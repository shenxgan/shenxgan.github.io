<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pandas 常用操作]]></title>
    <url>%2F2017%2F12%2F05%2Fpandas-basis%2F</url>
    <content type="text"><![CDATA[整理 pandas 常用操作整理记录工作学习中遇到的所有 pandas 操作 设置输出宽度http://pandas.pydata.org/pandas-docs/stable/options.html1pd.set_option('display.width', None) 设置成 None 后，在打印 dataframe 时，所有的列名将不会再换行，看起来会比较清晰、舒服 更改列名http://stackoverflow.com/questions/11346283/renaming-columns-in-pandas1df = df.rename(columns=&#123;'oldName1': 'newName1', 'oldName2': 'newName2'&#125;) dataframe 长度1len(df.index) 所有列的数据类型1df.dtypes mergehttp://pandas.pydata.org/pandas-docs/stable/merging.html1result = pd.merge(left, right, on=['key1', 'key2']) updatedf 和 df2 的 index 必须要保证数据类型一样，特别要注意 int 和 string；因为两者人眼并不能看出来，但是就因为数据类型不一样就会导致 update 总是没有效果1df.update(df2) append1df = df.append(df2) 读取 csv 文件https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html1df = pd.read_csv(csv_path) 保存为 csv 文件1df.to_csv('test.csv', header=True, index=False, mode='w', encoding="utf8") 如果 csv 文件中有中文（无论是表头还是内容），可以将 encoding 设置为 encoding=&quot;gbk&quot; groupby 之后求去重个数12df.groupby(index).agg(&#123;'field': lambda x: len(x.unique())&#125;)df.groupby(index).field.nunique() 检测是否为空12if df.empty: print('df is empty.') 填充 NA/NaNhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html将所有 NaN 替换为 0123df = df.fillna(0) # 填充为 0df = df[df.field.notnull()] # 去掉 field 列为 NaN 的行df = df.dropna() # 去掉任意列为 NaN 的行 sort 排序http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.sort.html默认升序，下列示例是以列 A 和 B 进行排序，其中 A 列升序，B 列降序1result = df.sort(['A', 'B'], ascending=[1, 0]) 去重https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html删除重复行，默认保留第一个1result = df.drop_duplicates() 新添一列新列与其它列没有关系1df['new_col'] = value 新列是通过其它列通过计算得出比如将时间戳转为日期格式，或者将 ip 转为地址又比如对于成绩表来说，我需要统计总分1df['new_col'] = df.apply(func, args=(arg1, arg2, ...)) 更改列名顺序假如 df 列名顺序为 [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]，改成 [&#39;D&#39;, &#39;C&#39;, &#39;B&#39;, &#39;A&#39;]：1df = df[['D', 'C', 'B', 'A']] 只取其中部分列1df = df[['A', 'D']] isin12df = df[df.field.isin(df2.field)]df = df[~df.field.isin(df2.field)] 逻辑运算123df2 = df[(df["key0"] == 0) &amp; (df["key1"] == 1)]df2 = df[(df["key0"] == 0) | (df["key1"] == 1)]df2 = df[~ (df["key0"] == 0)] set_index12df = df.set_index(['field1', 'field2'])df = df.reset_index() 聚合操作1df = df.groupby(['field1', 'field2']).agg(&#123;'field3': count, 'field3': lambda x: len(x.unique()), 'field4': sum&#125;) 遍历12for row in df.itertuples(): pass astype修改列类型1df['field'] = df['field'].astype(str)]]></content>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 读写 MySQL 数据库]]></title>
    <url>%2F2017%2F11%2F22%2Fspark-mysql%2F</url>
    <content type="text"><![CDATA[记录 spark 读写 mysql 数据库的方式spark 版本为 2.2.0 spark-shell 交互中连接 mysql官网的示例首先来看看官网上的示例： 1bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar 可以看到，官网上使用的是 postgresql 数据库作为的示例；mysql 其实也是一样的，下面就来介绍 spark 连接 mysql 的方法： 下载 mysql 相应的 jar 包可以看到上述连接 postgresql 数据库示例中，使用到了一个 jar 包：postgresql-9.4.1207.jar；相应的 mysql 也需要这样的一个 jar 包，可去官网下载；也可以在这里进行下载。 我下载了两个版本的进行了测试，分别为 mysql-connector-java-5.0.8-bin.jar、mysql-connector-java-6.0.6-bin.jar。这两个版本都没有问题，下面以 mysql-connector-java-5.0.8-bin.jar 为例。 启动 spark-shell1./bin/spark-shell --driver-class-path mysql-connector-java-5.0.8/mysql-connector-java-5.0.8-bin.jar --jars mysql-connector-java-5.0.8/mysql-connector-java-5.0.8-bin.jar 启动过程中如果报错，则尝试删除 metastore_db/dbex.lck 文件；再次运行即可https://stackoverflow.com/questions/37442910/spark-shell-startup-errors 1rm metastore_db/dbex.lck 读取 mysql 表数据https://docs.databricks.com/spark/latest/data-sources/sql-databases.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566scala&gt; val jdbcUsername = "root"jdbcUsername: String = rootscala&gt; val jdbcPassword = "xxxxxx"jdbcPassword: String = xxxxxxscala&gt; val jdbcHostname = "localhost"jdbcHostname: String = localhostscala&gt; val jdbcPort = 3306jdbcPort: Int = 3306scala&gt; val jdbcDatabase ="xblog"jdbcDatabase: String = xblogscala&gt; val connectionProperties = new Properties()&lt;console&gt;:23: error: not found: type Properties val connectionProperties = new Properties() ^scala&gt; import java.util.Propertiesimport java.util.Propertiesscala&gt; val connectionProperties = new Properties()connectionProperties: java.util.Properties = &#123;&#125;scala&gt; connectionProperties.put("user", jdbcUsername)res0: Object = nullscala&gt; connectionProperties.put("password", jdbcPassword)res1: Object = nullscala&gt; val jdbc_url = s"jdbc:mysql://$&#123;jdbcHostname&#125;:$&#123;jdbcPort&#125;/$&#123;jdbcDatabase&#125;"jdbc_url: String = jdbc:mysql://localhost:3306/xblogscala&gt; val df_student = spark.read.jdbc(jdbc_url, "student", connectionProperties)java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:83) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:34) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146) at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:193) ... 48 elidedscala&gt; Class.forName("com.mysql.jdbc.Driver")res2: Class[_] = class com.mysql.jdbc.Driverscala&gt; val df_student = spark.read.jdbc(jdbc_url, "student", connectionProperties)df_student: org.apache.spark.sql.DataFrame = [id: bigint, name: string ... 1 more field]scala&gt; df_student.show()+---+----+---+| id|name|age|+---+----+---+| 2| 张三| 18|| 4| 李四| 20|+---+----+---+scala&gt; 写入数据到 mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051scala&gt; case class Person(name: String, age: Long)defined class Personscala&gt; val df = Seq(Person("Andy", 32)).toDF()df: org.apache.spark.sql.DataFrame = [name: string, age: bigint]scala&gt; df.show()+----+---+|name|age|+----+---+|Andy| 32|+----+---+scala&gt; df.write.jdbc(jdbc_url, "student", connectionProperties)org.apache.spark.sql.AnalysisException: Table or view 'student' already exists. SaveMode: ErrorIfExists.; at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81) at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:472) at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92) at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233) at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:461) ... 48 elidedscala&gt; import org.apache.spark.sql.SaveModeimport org.apache.spark.sql.SaveModescala&gt; df.write.mode(SaveMode.Append).jdbc(jdbc_url, "student", connectionProperties)scala&gt; df_student.show()+---+----+---+| id|name|age|+---+----+---+| 2| 张三| 18|| 4| 李四| 20|| 6|Andy| 32|+---+----+---+scala&gt; 命令汇总总的一套下来，就是如下这些：1234567891011121314151617181920212223242526272829Class.forName("com.mysql.jdbc.Driver")// 数据库连接属性信息val jdbcUsername = "root"val jdbcPassword = "xxxxxx"val jdbcHostname = "localhost"val jdbcPort = 3306val jdbcDatabase ="xblog"import java.util.Propertiesval connectionProperties = new Properties()connectionProperties.put("user", jdbcUsername)connectionProperties.put("password", jdbcPassword)// 读取数据库表 studentval jdbc_url = s"jdbc:mysql://$&#123;jdbcHostname&#125;:$&#123;jdbcPort&#125;/$&#123;jdbcDatabase&#125;"val df_student = spark.read.jdbc(jdbc_url, "student", connectionProperties)df_student.show()// 创建一个 DataFrame// https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasetscase class Person(name: String, age: Long)val df = Seq(Person("Andy", 32)).toDF()df.show()// 存入数据库表import org.apache.spark.sql.SaveModedf.write.mode(SaveMode.Append).jdbc(jdbc_url, "student", connectionProperties)df_student.show() spark 程序中连接 mysql在未使用 mysql 之前，启动命令如下：1./bin/spark-submit --class "SimpleApp" --master spark://172.16.201.213:7077 target/scala-2.11/simple-project_2.11-1.0.jar 要支持 mysql ，启动命令如下：1./bin/spark-submit --driver-class-path mysql-connector-java-5.0.8-bin.jar --jars mysql-connector-java-5.0.8-bin.jar --class "SimpleApp" --master spark://172.16.201.213:7077 target/scala-2.11/simple-project_2.11-1.0.jar 可以看到，和前面的在 spark-shell 交互中一样，启动时添加了 --driver-class-path 参数和 --jars 参数。测试了一下，这两参数缺一不可，且必须在前头，还不能放在后面加入。build.sbt 文件并不需要修改。]]></content>
      <tags>
        <tag>spark</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客搭建记录]]></title>
    <url>%2F2017%2F11%2F04%2Fbuild-record%2F</url>
    <content type="text"><![CDATA[记录本博客搭建历程 基本框架Hexo 建站https://hexo.io/zh-cn/docs/setup.html 123$ hexo init hexo_xblog$ cd hexo_xblog/$ npm install 使用 NexT 主题http://theme-next.iissnan.com/getting-started.html 下载1$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启用打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 1$ vim _config.yml 1theme: next 本地运行1hexo s --debug next 主题提供了多个样式以供选择，我使用了 Mist 样式：1vim themes/next/_config.yml 1scheme: Mist 部署到 github.iohttps://hexo.io/zh-cn/docs/deployment.html 在站点配置文件中配置 deploy 字段1$ vim _config.yml 12345deploy: type: git repo: git@github.com:shenxgan/shenxgan.github.io.git branch: master message: 安装 hexo-deployer-git1$ npm install hexo-deployer-git --save 部署1$ hexo deploy 如果是重新部署，则最好先进行 clean，不然修改了配置后部署并不会生效。1$ hexo clean; hexo deploy 集成第三方服务http://theme-next.iissnan.com/third-party-services.html 搜索个人觉得 Local Search 就比较好用。具体使用方法官网已经写着很明细了，照着弄就行。http://theme-next.iissnan.com/third-party-services.html#local-search 评论选用的是 gitment 作为博客的评论系统。gitment 使用 github 的 issue 来达到评论的目的，个人觉得非常不错。 因为在 next 主题的配置文件中发现了 gitment， 说明 next 主题已经直接支持 gitment 了。所以没有再使用 gitment 作者这篇文章中介绍的方式来使用 gitment；而是直接在 next 主题配置文件中配置相应的参数即可。 安装 gitment1$ npm install --save gitment 启动 gitment 并添加配置1vim themes/next/_config.yml 12345678910111213gitment: enable: true mint: true count: true lazy: false cleanly: false language: github_user: shenxgan github_repo: shenxgan.github.io client_id: 5e1xxxxxxxxxx87c1 client_secret: 1c7xxxxxxxxxxxxxxxxxxx409b proxy_gateway: redirect_protocol: 千万注意不要将 github_user 和 github_repo 弄错 我在刚开始的时候弄错了，事情大概是这样的： github_user 以为是 github 中用户的 id，还使用这个 api (https://api.github.com/users/shenxgan) 来获取自己的 id 来着。 当时一直以为是 github_repo 的问题，因为刚开始我写这个值的时候是带着 git 的；后来查到这个 issue 中写的，又认为值应该是 shenxgan … 一直都不对 还好，最终搞了两天终于弄好了 (*^_^*) client_id 和 client_secret 可参照 gitment 作者写的：https://imsun.net/posts/gitment-introduction/#1-注册-OAuth-Application 修改主题样式其它的一些小改动首页中显示“阅读全文”在文章中添加 &lt;!-- more --&gt; 来精确控制摘要内容；会将 &lt;!-- more --&gt; 之前的内容定位为摘要；且在阅读文章的时候并不会显示这个标签还有一个优点是可以保留样式]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>mist</tag>
        <tag>gitment</tag>
        <tag>github.io</tag>
        <tag>Hexo 博客</tag>
        <tag>NexT 主题</tag>
        <tag>阅读全文</tag>
      </tags>
  </entry>
</search>
